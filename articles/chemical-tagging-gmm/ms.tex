%% This document is part of the snob project
%% Copyright 2017 the authors. All rights reserved.

\documentclass{aastex61}

\usepackage{bm}
%\usepackage{boldsymbol}
\usepackage{verbatim} % for \begin{comment}'ing out sections.

% For revision history
\IfFileExists{vc.tex}{\input{vc.tex}}{
    \newcommand{\githash}{UNKNOWN}
    \newcommand{\giturl}{UNKNOWN}}

% Define commands
\newcommand{\article}{\emph{Article}}
\newcommand{\acronym}[1]{{\small{#1}}}
\newcommand{\project}[1]{\textsl{#1}}

% Surveys
\newcommand{\apogee}{\acronym{APOGEE}}
\newcommand{\ges}{\acronym{GES}}
\newcommand{\hermes}{\acronym{HERMES}}
\newcommand{\galah}{\acronym{GALAH}}
\newcommand{\fourmost}{\acronym{4MOST}}
\newcommand{\weave}{\acronym{WEAVE}}
\newcommand{\gaia}{\project{Gaia}}

% Common terms
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}

\def\teff{T_{\rm eff}}
\def\logg{\log{g}}

% Have some maaaath.
\def\infinity{\rotatebox{90}{8}}
\def\veccov{\vect{C}}
\def\vecmean{\vect{\mu}}
\def\vectheta{\vect{\theta}}
\def\weight{w}
\def\weights{\vect{\weight}}
\def\datum{y}
\def\data{\vect{\datum}}
\def\likelihood{\mathcal{L}}

% Affiliation(s)
\newcommand{\moca}{
	\affil{School of Physics and Astronomy, Monash University, 
		   Melbourne, Clayton VIC 3800, Australia}}
\newcommand{\claytonfit}{
	\affil{Faculty of Information Technology, Monash University,
	       Melbourne, Clayton VIC 3800, Australia}}
\newcommand{\caulfieldfit}{
	\affil{Faculty of Information Technology, Monash University,
		   Melbourne, Caulfield East VIC 3145, Australia}}



\received{}
\revised{}
\accepted{}

\submitjournal{MNRAS} 

\shorttitle{Chemical Tagging Using Minimum Message Length. I.}
\shortauthors{Casey et al.}

\begin{document}

\title{Chemical Tagging Using Minimum Message Length. I.\\
       Gaussian Mixture Modelling}

\correspondingauthor{Andrew R. Casey}
\email{andrew.casey@monash.edu}

\author[0000-0003-0174-0564]{Andrew R. Casey}
\moca
\claytonfit

\author{(some order of:}

\author[0000-0003-2952-859X]{John Lattanzio}
\moca

\author[0000-0002-0583-5918]{David Dowe} 
\claytonfit

\author[0000-0002-1716-690X]{Aldeida Aleti}
\caulfieldfit

\author{)}

\author{others?}
 
\begin{abstract}
Chemical tagging seeks to identify co-natal stars by their present-day 
photospheric abundances, long after their phase space similarity is lost.
In principle the detailed chemical abundances of a transformative number of
stars could be used to reconstruct the evolution of the Milky Way, providing
insight on supernovae yields, galactic dynamics, the initial mass function, 
as well as the formation and destruction of star clusters. 

Some progress has been made towards chemical tagging, but 
here we argue that even the \emph{extent} of the problem has not been realised.
We introduce the Real World problems associated with chemical tagging at scale,
and describe how currently considered methods will inevitably fail even under
the most unrealistically optimistic conditions. We introduce the minimum message
length to the astronomical community as a promising alternative to chemical
tagging, one which has the capability to address all ensivaged problems with
chemical tagging simultaneously. We perform chemical tagging experiments with
infinte Gaussian mixture models, and demonstrate using real and generated data 
that MML outperforms all other considered penalty techniques. 
\end{abstract}

\keywords{}

\section{Introduction} 
\label{sec:introduction}

\citet{Freeman_2002} introduced the idea of chemical tagging to identify 
groups of stars that formed together in the same gas cloud, which are no 
longer identifiable from their physical proximity or orbital properties.
This idea is attractive because the observable chemical abundances of 
stars remains largely unchanged throughout a star's lifetime, whereas phase 
space similarity can be quickly washed out through dynamical interactions. 
Indeed, although most stars are thought to form in clusters \citep{someone},
only the most massive or isolated star clusters are able to survive the star
formation process \citep[e.g.,][]{Lada_2010}, leaving most clusters to become 
dynamically unbound after about 1~Gyr. 

%Their signature of formation is locked up in their detailed chemical fingerprint

Despite the Milky Way being our best laboratory for understanding galaxy
formation, a transformative number of stars with precise abundances will be
required to chemically tag the Galaxy.
Ground- and space-based surveys are poised to deliver those data in the coming
decade. 
The Apache Point Observatory Galactic Evolution Experiment 
\citep[\apogee, e.g.,][]{apogee} and the Gaia-ESO survey 
\citep[\ges;][]{Gilmore_2012,Randich_2013} are already delivering $10^5$ stars 
with 20--40 precise abundances, and the Galactic Archaeology with \hermes\ 
\citep[\galah;][]{DeSilva_2015} survey seeks to derive $\approx$30 precise 
abundances for $\sim10^6$ stars. 
Future ground-based surveys will derive abundances from $\approx$20 million 
spectra in each hemisphere \citep[e.g., \weave, \fourmost;][]{weave,4most},
and the radial velocity spectrometer on \gaia\ can deliver a few abundances 
for $\approx$150 million stars.


Chemical tagging is a simple idea, but with this volume of data it represents
a formidable parameter estimation and model selection problem.
Given the difficulty of the problem, most chemical tagging studies have 
focussed on addressing one \emph{aspect} of chemical tagging.
For example, even the simplest applications of chemical tagging requires the
identification of groups in the data, with stars being assigned to certain
groups (a clustering problem), and to somehow chose the number of groups that
are best represented by the data (a model selection problem).
Previous works include definitions of L1-like metrics \citep{Mitschang_2014},
where open clusters are used to define confidence intervals of similarity
\citep{Mitschang_2013}, and whether similar stars constitute a group or not.
Other studies have directly applied standard clustering algorithms,
\citep[e.g., \acronym{DBSCAN}, $k$-means;][]{Blanco_Cuaresma_2015,Hogg_2016} 
and fixed the number of clusters.
Inferring the number of true clusters -- one of the principal goals of 
chemical tagging -- was not attempted in these simplified experiments.


Other works have focussed [TODO EXPAND]:
\begin{itemize}
  \item Dimensionality of chemical abundance space (Ting et al).
  \item Chemical homogenity of open clusters (Bovy, Da Silva)
  \item Dopplegangers (Ness)
  \item A single common latent factor, integrated over cosmic time (Bland-Hawthorn)
\end{itemize}

The aforementioned chemical tagging experiments have sought to address only 
individual challenges associated with chemical tagging.
These challenges are all related, and therefore addressing them separately
will give results that ado not make use of all
the information available. 
Here, as we introduce the problem of chemical tagging, we list the inferences
we wish to make from a large number of chemical abundances.


\subsection{What's the goal of chemical tagging? What do we want to infer?}
\label{sec:the-problem}


Let us assume that a set of stars have been observed spectroscopically, and
chemical abundances have been derived from those data.
We will unrealistically assume that the data are noiseless, and the derived
abundances are unbiased, known with infinite precision, and complete (e.g., no 
missing abundances).
Regardless of the number of stars observed or abundances derived, there are a
number of inferences we seek to make:


\begin{enumerate}
  \item The number of star clusters (or star-forming events) where at least 
        one star was observed and chemical abundances were derived.
  \item The conditional probabilities (or memberships) of each star belonging
        to every star cluster or star-forming event.
        Alternatively, the relative weighting of individual mixtures.
  \item The mean abundances of each star cluster. 
  \item The covariance matrix of abundances of each star cluster.
        This covariance matrix includes the intrinsic (cluster) variance
        of individual abundances (e.g., the homogeniety), and allows for
        correlations between abundances.\footnote{
          It is likely -- or hoped -- that \emph{true} correlations between 
          abundances will be accounted for through the latent factors, and any 
          remaining correlations are the result of the measurements.}
  \item The \emph{number} of common multivariate latent factors, variables 
        that are not observable but affect all or many of the observations.
        In the context of chemical tagging, the yields of core-collapse
        supernovae can be considered a common multivariate latent factor: 
        those yields are not directly observable, but they contribute to much 
        of the data.
        Here we are interested in the \emph{number} of common multivariate
        latent factors. For example, the number of sources of enrichment:
        different classes of supernovae, asymptotic giant branch (AGB) stars, 
        etc.
  \item The multivariate factor loads of individual latent factors. 
        For example, if core-collapse supernovae and AGB stars had both 
        contributed to the chemical abundances of all stars in the sample, 
        then the \emph{factor loads} would be the \emph{yield} of the 
        supernovae, or the yield of relative abundances that were produced by 
        the AGB star. 
        Each star will be affected differently by those yields (see item 7).
  \item The relative \emph{scoring} of different latent factors on each star.
        This is a scaling that is applied to the latent yields. Continuing our
        analogy on yields from core-collapse supernovae and AGB stars, the
        \emph{factor scores} can be thought of as the relative contributions 
        of the different factor loads, for a single star.
  \item The intrinsic \emph{specific variances} of the individual abundances.
        These variances can be thought of as the intrinsic variability from 
        the different factor loads.
        The intrisic variances affect all observations, before accounting for
        any homogeneity in individual clusters.
  \item The \emph{number} of stellar age- or parameter-dependent latent 
        factors that affect the data. 
        The surface chemical abundances of stars \emph{do change} over its
        lifetime, at least in part due to atomic diffusion, veiling, and 
        thermohaling mixing. % TODO: Add Korn, others?
        Understanding and modelling these effects will be critical for 
        chemical tagging stars across all stellar ages and evolutionary states.
  \item The factor loads of stellar age- or parameter-dependent latent factors.
        In this example, a single multivariate factor load may represent the
        age-dependent effects of atomic diffusion.
  \item The factor scores of stellar age- or parameter-dependent latent 
        factors.
\end{enumerate}


The number of things we seek to infer from the available chemical abundances
is vast.
Indeed, even if we ignore the latent factors then a non-trivial model
selection problem remains: What is the number of star clusters or star-forming 
events?
In the full description of chemical tagging we seek to know (1) the number of
star clusters, (2) the number of latent factors, and (3) the number of age-
or stellar parameter-dependent factors.
Going further, if we wanted to improve the accuracy and precision of our
solution then we may seek to include joint priors on the memberships of stars
being associated with specific clusters, either based on their ages or
astrometry.
In summary, simplified versions of chemical tagging represent formidable model
selection problems with non-convex\footnote{
  If a problem is convex then any local solution is mathematically guaranteed
  to be the global solution.
  If the problem is non-convex, then a local solution may not be the global
  solution.
} objective functions.





In this \article\ we will introduce a simple probabilistic approach to
chemical tagging, where the data are modelled as a mixture of multivariate
Gaussian distributions and the number of mixtures is unknown.
In Section \ref{sec:the-model} we describe our model and outline our objective
function.
Although our model is simplistic in the sense that we will only address some
aspects of chemical tagging, in Section \ref{sec:experiments} our experiments
show that our approach is superior to all other methods considered 
for chemical tagging.
In Section \ref{sec:discussion} we describe how the principles introduced here
can be used to construct a fully consistent probabilistic approach to chemical
tagging, and simultaneously address the challenges outlined in Section 
\ref{sec:the-problem}.


\section{The Model}
\label{sec:the-model}


We make the following explicit assumptions:

\begin{itemize}
  \item We assume that the data (e.g., $D$ detailed chemical abundances for
        $N$ stars) can be represented by a mixture of $K$ multivariate 
        Gaussian distributions.
  \item We assume that there are no missing data values. 
        For example, if a star has been observed, then there is a complete set
        of $D$ chemical abundances available.
  \item We will assume that the number of \emph{true} multivariate Gaussian
        distributions $K_{true}$ is not known. 
        This assumption applies to our experiements involving generated data, 
        and those using real data.
  \item In addition to not knowing the \emph{number} of multivariate normal
        distributions, we further assume that the means $\vecmean$ and 
        covariance matrices $\veccov$ of each $K$-th mixture is unknown.
  \item We assume that the covariance matrix of individual components can be
        described as `free' or `full', insomuch that the data within a cluster
        can have off-diagonal correlation terms, and those terms are unknown.
  \item In all experiments we will assume that the data have homoskedastic 
        noise properties.
  \item The relative weights $\weights$ (or mixing proportions) of the $K$ 
        distributions is unknown. 
  \item We assume a relative (or conditional) probability distribution for 
        each star belonging to a given cluster. That is to say that we do not 
        adopt a `\emph{hard selection}' approach to cluster modelling, where 
        stars would be assigned as definitively belonging to one cluster or 
        another. 
        Our approach could be described as `\emph{soft} clustering'.
        This assumption has practical implications for optimization, and for
        the total message length.
\end{itemize}


\noindent{}The probability density function $f$ for a multivariate normal 
distribution with $D$ dimensions is given by

\begin{equation}
  f(\data|\vecmean,\veccov) 
      = \frac{1}{\sqrt{(2\pi)^d|\veccov|}}
        \exp{\left[-\frac{1}{2}(\data - \vecmean)^\intercal\veccov^{-1}(\data - \vecmean)\right]}
\end{equation}

\noindent{}where $\vecmean$ and $\veccov$ is the multivariate mean and 
covariance matrix, and $\data$ are the data.
For a fixed number of $K$ multivariate Gaussian mixtures, the probability
density function can be written as

\begin{equation}
  f(\data|\vectheta) = \sum_{k=1}^{K} \weight_k \, f(\data|\vectheta_k)
\end{equation}

\noindent{}where $\vectheta_k \equiv \{\vecmean_k, \veccov_k\}$,
$\vectheta \equiv \{\vectheta_1,\dots,\vectheta_K,\weight_1,\dots,\weight_K\}$, and
$\weight_k$ is the relative weight (or the mixing probability) of the $k$-th 
mixture,\footnote{
  An alternative representation would be to include integer variables for each
  of the $N$ stars, where each variable represents the cluster which that star
  belongs.
  These two models are mathematically identical \citep[e.g., see discussion in][]{Foreman_Mackey_2014},
  but the integer representation is highly dimensional and difficult to
  optimize.}
and we require that $\sum_{k=1}^{K} \weight_k = 1$.
Although the true number of $K$ mixtures is unknown (as are the means $\vecmean$
and covariance matrices $\veccov$), we will address the selection of $K$
and parameter estimates of $\vectheta$ in Section \ref{sec:fitting-the-data}.


\subsection{Minimum Message Length}
\label{sec:mml}

The Minimum Message Length \citep[MML;][]{Wallace_1968} principle is a formal
description of Occam's razor that is based on information theory.
The MML principle states that the best explanation of the data is the model
that describes the data using the smallest amount of information (shortest
message).
Information is described with the same description as \citet{Shannon_1948},
who showed that that given an event $E$ with probability $P(E)$, the length of 
the shortest lossless code $I(E)$ to represent that event requires 
$I(E) = -\log_{2}P(E)$ bits of information\footnote{
  Unless specified -- as it is in this instance -- throughout this article we 
  will refer to the information $I$ or entropy in natural units (nats). 
  Recall that 1 bit $= \log{2}$ nats.
  }.
\citet{Wallace_1968} linked this result with Bayes' theorem, 

\begin{equation}
  P(H|D) = \frac{P(H)\cdot{}P(D|H)}{P(D)}
\end{equation}

\noindent{}which states that the joint probability of the data $D$ and 
hypothesis $H$, $P(H,D)$, is given by

\begin{equation}
  P(H,D) = P(H) \cdot P(D|H) = P(D) \cdot P(H|D)
\end{equation}

\noindent{}where $P(H)$ is the prior probability of the hypothesis $H$,
$P(D|H)$ is the likelihood, $P(D)$ is the prior probability of the data $D$,
and $P(H|D)$ is the posterior probability of the hypothesis given the data.
Specifically, \citet{Wallace_1968} derived the relationship between
conditional probabilities in terms of optimal message lengths,

\begin{equation}
  I(H,D) = I(H) + I(D|H) = I(D) + I(H|D) \quad .
  \label{eq:mml-wallace}
\end{equation}

Therefore the total cost of a message is the sum of the message length of the
hypothesis $H$, which takes $I(H)$ bits, and the data $D$ given the hypothesis
$H$, $I(D|H)$. 
Considering a simple problem of model selection provides good intuition for
this two-part message approach. 
Consider a simple model $H_0$ where the first part of the message $I(H_0)$ is 
short, and which provides a poor fit to the data such that $I(D|H_0)$ is long.
An alternative model $H_1$ is more complex, requiring a longer $I(H_1)$, but
the increased model flexibility provides a better fit to the data, making
$I(D|H_1)$ short.
For both parameter estimation \emph{and} model selection, we want to chose
$H$ (of $H_0$, $H_1$, etc) and the model parameters that minimizes the
\emph{total} message length.



Calculating the length of the message is a non-trivial task, especially if the
model is even reasonably complex.
This makes the MML principle intractable (or uncomputable) in most cases, and
forces us to make approximations when calculating the message length.
Using a Taylor expansion, \citet{Wallace_1987} introduce a generalised 
scheme to estimate a parameter vector $\vectheta$ (of any distribution), that
minimises the message length expression:
\begin{equation}
\label{eq:message-length}
  I(\vectheta,\data) 
    = \frac{Q}{2}\log\kappa{\left(Q\right)} 
    - \log{\left(\frac{p(\vectheta)}{\sqrt{|\mathcal{F}(\vectheta)|}}\right)}
    - \likelihood\left(\data|\vectheta\right) 
    + \frac{Q}{2} \quad .
\end{equation}


Here $p(\vectheta)$ is the prior on the hypothesis, $q$ is the number of free
parameters in the model, $\kappa\left(Q\right)$ is a function to approximate the lattice
quantisation constant for $Q$ free parameters \citep[e.g.,][]{Conway_1984}, 
and $|\mathcal{F}\left(\vectheta\right)|$ is the determinant of the 
\emph{expected} Fisher information matrix (the second-order partial 
derivatives of the negative log-likelihood function 
$-\likelihood(\data|\vectheta)$).




%The message length for a model includes the length (in bits of entropy)
%required to transmit the model parameters, and the length required to transmit
%the residuals of the data given the model.
%There is a wealth of literature on the successes (and derivatives) of MML
%in the fields of medicine, statistics, etc.

%- Deep grounds in statistics, etc.


The MML principle requires that the message fully specifies all components
necessary for the receiver to be able to reconstruct the message, as well as
our prior beliefs of the model.
The message must include:

%This requirement provides a familiar link to Bayesian statistics, and allows
%one to think of MML as a form of a Bayesian optimization problem that is 
%deeply grounded in information theory, with successful contributions in
%economics, medicine, statistics, etc \citep{citations_needed}.


\begin{enumerate}
  \item The number of Gaussian mixtures, $K$.
  \item Encoding the relative weights $\weights$ of the $K$ Gaussian mixtures.
  \item Encoding the component parameters $\vecmean$, $\veccov$ for all $K$
        Gaussian mixtures.
  \item Encoding the data, given the model parameters.
  \item The lattice quantisation constant $\kappa(Q)$ for the number of model
        parameters $Q$.
\end{enumerate}

Some of these components require non-trivial derivations or approximations in
order to calculate the length of the optimal loseless encoding. 
For this reason, we will consider the message length of each component in turn
before returning to the full message length specified in Equation \ref{eq:message-length}.


\subsubsection{Encoding the number of Gaussian mixtures, $K$}
\label{sec:encoding-k}


We assume that fewer mixtures are more likely than a larger number of mixtures.
Specifically, we assume that $p(K) \propto 2^{-K}$, which implies that the
optimal message length required to encode $K$ is given by:

\begin{equation}
  I(K) = -\log{p(K)} = K\log{2} + \textrm{constant} \quad .
\end{equation}


\subsubsection{Encoding the relative weights, $\weights$}
\label{sec:encoding-weights}

We assume a uniform prior on the mixing weights $\bm{w}$, only requiring that
$\sum_{k=1}^{K}w_k = 1$. 
This implies that the weights can be treated as parameters of a multinomial
distribution, and the length of their optimally encoded message is given by
\citet{Boulton_1969},

\begin{equation}
  I(\weights) 
    = \frac{K - 1}{2}\log{N} 
    - \frac{1}{2}\sum_{k=1}^{K}\log\weight_k 
    - \log{\Gamma{\left(K\right)}} \quad ,
\end{equation}

\noindent{}where $\Gamma(K)$ is the usual gamma function for positive integers
$\Gamma(K) = (K - 1)!$.
A useful extension of this work might be to impose a Dirchlet prior on the 
mixing weights $\weights$, or a distribution inspired by the stellar mass 
function and/or an observed selection function.


\subsubsection{Encoding the mixture parameters $\vecmean$ and $\veccov$}
\label{sec:encoding-mixture-parameters}


In order to properly encode the mixture parameters $\vecmean$ and $\veccov$
for all $K$ mixtures, we must encode both our prior belief on those parameters,
and the determinant of the expected Fisher information matrix.
For the $k$-th mixture this becomes,

\begin{equation}
  I(\vecmean_k,\veccov_k) = -\log{\left(\frac{p(\vecmean_k,\veccov_k)}{\sqrt{|\mathcal{F}\left(\vecmean_k,\veccov_k\right)|}}\right)}
                          = -\log{p(\vecmean_k,\veccov_k)} + \frac{1}{2}\log{|\mathcal{F}\left(\vecmean_k,\veccov_k\right)|}
\end{equation}

\noindent{}and for all mixtures:

\begin{equation}
  I(\vecmean,\veccov) = -\sum_{k=1}^{K}\log{p(\vecmean_k,\veccov_k)} + \frac{1}{2}\sum_{k=1}^{K}\log{|\mathcal{F}\left(\vecmean_k,\veccov_k\right)|}
  \label{eq:10}
\end{equation}

We introduce our priors on $\vecmean$ and $\veccov$ before approximating the
expected Fisher information matrix.
We adopt a uniform prior of $\mathcal{U}(\vecmean) = [-\infinity, \infinity]$ 
on all multivariate mean abundances $\vecmean$ (in all $D$ dimensions).
We note that while this prior is improper, it becomes proper when it enters
the (accepted) posterior distribution, and it has no impact on our inferences.
Indeed, we similarly could adopt a (proper and) large uniform prior on all 
mean abundances of $\mathcal{U}(\vecmean) = [-12, 12]$ (24 orders of magnitude!)
and our inference would be the same.

We adopt a congujate inverted Wishart prior for the covariance matrices in the
individual mixtures. 
The joint prior density for the parameters of a single
mixture is given by the limiting form of the normal inverted-Wishart
density \citep[e.g., Section 5.2.3 of ][]{Schafer_1997}:

\begin{equation}
    p(\vecmean_k,\veccov_k) \propto |\veccov_k|^{\frac{D + 1}{2}} \quad .
    \label{eq:prior-mean-cov}
\end{equation}


Computing the expected Fisher information $\mathcal{F}(\vecmean_k,\veccov_k)$ 
requires the second order partial derivatives of the negative log-likelihood 
$-\likelihood(\data|\vecmean,\veccov)$.
We approximate the determinant of the Fisher information 
$|\mathcal{F}(\vecmean_k, \veccov_k)|$ as the product of 
$|\mathcal{F}\left(\vecmean_k\right)|$ and $|\mathcal{F}\left(\veccov_k\right)|$ 
\citep{Oliver_1996,Roberts_1998}. 
We begin by taking second derivative of the log-likelihood function
$\likelihood\left(\data|\vecmean,\veccov\right)$,

\begin{equation}
  \likelihood\left(\data|\vecmean, \veccov\right) = 
  -\frac{Nd}{2}\log{\left(2\pi\right)} - \frac{N}{2}\log{|\veccov|} - \frac{1}{2}\Sigma_{i=1}^{N}\left(\data_i - \vecmean\right)^\intercal{}\veccov^{-1}\left(\bm{y}_i - \vecmean\right)
\end{equation}

\noindent{}with respect to $\vecmean$,

\begin{eqnarray}
  \frac{\delta\likelihood}{\delta\vecmean} = \Sigma_{i=1}^{N}\veccov^{-1}(\data_i - \vecmean)
  & \hspace{1em}\textrm{and}\hspace{1em}
  & \frac{\delta^2\likelihood}{\delta\vecmean^2} = -N\veccov^{-1} \quad ,
\end{eqnarray}

\noindent{}and for a $D$-dimensional vector $\vecmean$ the Fisher information
matrix is:

\begin{eqnarray}
    |\mathcal{F}(\vecmean_k)| = \left|-\frac{\delta^2\likelihood}{\delta\vecmean^2}\right| = (N\weight_k)^D|\veccov|^{-1} \quad .
  \label{eq:approx-fisher-mean}
\end{eqnarray}

Computing the Fisher information of the covariance matrix 
$|\mathcal{F}\left(\veccov\right)|$ is harder.
Using theory of matrix derivatives based on matrix vectorization \citep{Dwyer_1967},
\citet{Magnus_1988} derived the following analytical expression for the 
determinant of the Fisher information for a covariance matrix
$|\mathcal{F}\left(\veccov_k\right)|$,


\begin{equation}
    |\mathcal{F}\left(\veccov_k\right)| = (N\weight_k)^\frac{D(D+1)}{2}2^{-D}|\veccov_k|^{-(D+1)}
  \label{eq:approx-fisher-cov}
\end{equation}

\noindent{}where $D(D+1)/2$ arises from the number of free parameters in a
covariance matrix with off-diagonal terms.
Equations \ref{eq:approx-fisher-mean} and \ref{eq:approx-fisher-cov} allow us
to approximate the determinant of the expected Fisher information matrix
$|\mathcal{F}\left(\vecmean_k,\veccov_k\right)|$ as:

\begin{eqnarray}
  |\mathcal{F}\left(\vecmean_k,\veccov_k\right)| & \approx & |\mathcal{F}\left(\vecmean_k\right)|\cdot|\mathcal{F}\left(\veccov_k\right)| \nonumber \\
    |\mathcal{F}\left(\vecmean_k,\veccov_k\right)| & \approx & (N\weight_k)^{D}|\veccov_k|^{-1}(N\weight_k)^\frac{D(D+1)}{2}2^{-D}|\veccov_k|^{-(D+1)} \nonumber \\
    |\mathcal{F}\left(\vecmean_k,\veccov_k\right)| & \approx & (N\weight_k)^\frac{D(D+3)}{2}2^{-D}|\veccov_k|^{-(D+2)} \quad .
  \label{eq:fisher-mean-cov}
\end{eqnarray}

Thus for all $K$ mixtures, substituting Equations \ref{eq:prior-mean-cov} and 
\ref{eq:fisher-mean-cov} into Equation \ref{eq:10} gives:

\begin{eqnarray}
I(\vecmean,\veccov) &=& -\sum_{k=1}^{K}\log{p(\vecmean_k,\veccov_k)} + \frac{1}{2}\sum_{k=1}^{K}\log{|\mathcal{F}\left(\vecmean_k,\veccov_k\right)|} \nonumber  \\
I(\vecmean,\veccov) &=& -\sum_{k=1}^{K}\log{\left(|\veccov_k|^{\frac{D + 1}{2}}\right)}
    + \frac{1}{2}\sum_{k=1}^{K}\log{\left((N\weight_k)^\frac{D(D+3)}{2}2^{-D}|\veccov_k|^{-(D+2)}\right)} \nonumber \\
I(\vecmean,\veccov) &=& -\sum_{k=1}^{K}\frac{D + 1}{2}\log|\veccov_k|
 + \frac{1}{2}\sum_{k=1}^{K}\left[\frac{D(D+3)}{2}\log{N\weight_k} -D\log{2} -(D+2)\log|\veccov_k|\right] \nonumber \\
I(\vecmean,\veccov) &=& -\sum_{k=1}^{K}\frac{D + 1}{2}\log|\veccov_k|
 + \frac{K}{2}\left[\frac{D\left(D+3\right)}{2}\log{N\weight_k} - D\log{2}\right]
 -\sum_{k=1}^{K}\frac{D+2}{2}\log|\veccov_k| \nonumber \\
I(\vecmean,\veccov) &=& \frac{1}{2}\sum_{k=1}^{K}\log|\veccov_k|
 + \frac{K}{2}\left[\frac{D\left(D+3\right)}{2}\log{N\weight_k} - D\log{2}\right] \quad .
\end{eqnarray}



\subsubsection{Encoding the data, given the model parameters}
\label{sec:encoding-data}


Each datum can only be stated with a precision which is given by the accuracy
of the measurement.
If we assume homoskedastic noise properties for the observed data, and the
precision that each elemental abundance can be measured is $\mathcal{E}$, then
the \emph{probability} of a datum is given by 
$\textrm{Pr}(\datum_n) = \mathcal{E}^D\textrm{Pr}(\datum_i|\mathcal{M})$
where $\mathcal{M}$ represents the model and $\textrm{Pr}(\datum_i|\mathcal{M})$
is the \emph{probability density}.
Note that $\mathcal{E}$ is a constant, and is only used to maintain validity 
between \emph{probability} and \emph{probability density}.
For these experiments we adopt $\mathcal{E} = 0.01$, but note that the value of
$\mathcal{E}$ has no effect on our inferences.

The length of the total encoding of a datum given the model is,
\begin{equation}
  I(\data_n) = -\log{\textrm{Pr}(\data_n)} = -D\log\mathcal{E} - \log\sum_{k=1}^{K}\weight_{k}f_{k}(\data_n|\vecmean_k,\veccov_k)
\end{equation}

\noindent{}and for the entire data:

\begin{eqnarray}
  I(\data|\vectheta) &=& -ND\log\mathcal{E} - \sum_{n=1}^{N}\log\sum_{k=1}^{K}w_{k}f_k(\data_n|\vecmean_k,\veccov_k) \quad .\\
%  I(\data|\vectheta) &=& -ND\log\mathcal{E} - \sum_{k=1}^{K}\log{\weight_k} - \sum_{n=1}^{N}\sum_{k=1}^{K}f_k\left(\data_n|\vecmean_k,\veccov_k\right) \quad .
\end{eqnarray}


\subsubsection{Encoding the lattice constant}
\label{sec:encoding-lattice-constant}

The lattice quantisation constant arises from the approximation to the (so-called)
strict MML, where parameters are quantised into intervals in high dimensional
space.
First we will calculate the total number of free parameters in our model.
Recall that we assume our covariance matrices have non-zero off-diagonal terms,
requiring $\frac{KD(D+1)}{2}$ parameters for all $K$ covariance matrices
(where we implicitly assume $D > 1$).
The $K$ $D$-dimensional vector means require a total of $KD$ parameters.
Under the assumption $K > 1$, the $K$ relative weights $\weights$ only require
$K - 1$ parameters because $\sum_{k=1}^{K}\weight_k = 1$.
Therefore the total number of free parameters $Q$ is given by,

\begin{eqnarray}
  Q &=& \frac{KD(D+1)}{2} + KD + K - 1 \nonumber \\
  Q &=& K\left[\frac{D\left(D+3\right)}{2} + 1\right] - 1 \quad .
  \label{eq:q}
\end{eqnarray}

We use an approximation for the logarithm of the lattice constant
\citep[see Sections 5.2.12 and 3.3.4 of ][]{Wallace_2005} where,

\begin{eqnarray}
    I(y) \approx - \log\frac{p(\theta')}{\sqrt{F(\theta')}} 
                   - \log{f(y|\theta')}
                   - (Q/2)\log{2\pi}
                   + \frac{1}{2}\log{(\pi{}Q)}
                   - 1
\end{eqnarray}

\noindent{}such that,
\begin{eqnarray}
  \log\kappa(Q) &=& \frac{\log{Q\pi}}{Q} - \log{2\pi} - 1
\end{eqnarray}

\noindent{}and the expected error on $\log\kappa(Q)$ is less than 0.1~nit.
The $Q$ (lattice) terms in Equation \ref{eq:message-length} becomes:

\begin{eqnarray}
\frac{Q}{2}\left(\log\kappa\left(Q\right) + 1\right) &=& \frac{Q}{2}\left(\frac{\log{Q\pi}}{Q} - \log{2\pi} - 1 + 1\right) \nonumber \\
\frac{Q}{2}\left(\log\kappa\left(Q\right) + 1\right) &=& \frac{1}{2}\left(\log{Q\pi} - Q\log{2\pi}\right) \quad .
\end{eqnarray}


\subsection{The Objective Function}
\label{sec:objective-function}


The message lengths of individual components allows us to fully specify an
objective function to minimise the total message length, given any number of
Gaussian mixtures $K$ and parameter estimates $\vectheta$:

\begin{equation}
  I(\vectheta,\data) = I(K) + I(\weights) + \sum_{k=1}^{K}I(\vectheta_k) + I(\data|\vectheta) + \frac{1}{2}\left(\log{Q\pi} - Q\log{2\pi}\right)\quad .
\end{equation}

\noindent{}Substituting the expressions outlined in previous subsections 
(except for $Q$, as per Equation \ref{eq:q}, which we leave unexpanded
for brevity), we arrive at our complete objective function:

\begin{eqnarray}
  I(\vectheta,\data) &=&
      K\log{2} % I(K)
    + \frac{K - 1}{2}\log{N} - \frac{1}{2}\sum_{k=1}^{K}\log\weight_k - \log{\Gamma(K)} % I(\weights) 
    + \frac{1}{2}\sum_{k=1}^{K}\log{|\veccov_k|} \nonumber \\ % 
   &+&\frac{K}{2}\left[\frac{D(D+3)}{2}\log{N\weight_k} - D\log{2}\right] % I(\vectheta_k) = I(\vecmean_k,\veccov_k)
    - ND\log\mathcal{E} - \sum_{n=1}^{N}\log\sum_{k=1}^{K}w_{k}f_k(\data_n|\vecmean_k,\veccov_k) % I(\data|\vectheta)
    + \frac{1}{2}\left(\log{Q\pi} - Q\log{2\pi}\right) \quad . % lattice
    \label{eq:objective-function}
\end{eqnarray}


Our objective function includes the number of mixtures, $K$, a discrete valued
parameter.
We seek the number of mixtures $K$ and the parameter estimates $\vectheta$
that minimise our objective function (Eq. \ref{eq:objective-function}).

Here we will outline our optimization procedure to minimise the message length
when $K$ is fixed, before describing our search strategy to move between trial 
values of $K$.

[TODO]
- Expectation maximization, using MML (unbiased) updates

\subsection{The Search Strategy}
\label{sec:search-strategy}


\section{Experiments}
\label{sec:experiments}

Here we describe simple chemical tagging experiments that are so optimistic
that they could be considered laughable, but ones which will still fail for
even the most simplistic chemical tagging approaches.

Each experiment will edge closer to a representative example of chemical
tagging.

\subsection{Experiment 0: Position and Velocity information}

\subsection{Experiment 1: Using clusters, full complement of chemical abundances}

\subsection{Experiment 2: Using all clusters, full complement of chemical abundances, random fractions of field stars}

\subsection{Experiment 3: Using clusters, limited set of chemical abundances}

\subsection{Experiment 4: Fake data to replicate clusters, but use latent factor}

\subsection{Experiment 5: Fake data to replicate clusters, but use multiple latent factors}

\section{Discussion}
\label{sec:discussion}


\acknowledgments

This research has made use of NASA's Astrophysics Data System.
This research is supported by an Australian Research Council Discovery Project
grant (DP160100637). 
Source code for this project is available at \texttt{\giturl}. This document
was compiled on \gitdate\ from revision hash \texttt{\githash} in that
repository. 


\software{astropy \citep{Robitaille:2013}, numpy, scipy, scikit-learn} 

\begin{thebibliography}{}

% Note: I include the DOI as a comment on the same line after every \bibitem
%       entry. I do this in case I ever need to extract full bibstems for the
%       bibliography, which can be done using this curl command and the DOI:

%       curl -LH "Accept: application/x-bibtex" http://dx.doi.org/10.5555/12345678 

%       If no DOI is available then I list the ADS identifier.

\bibitem[Astropy Collaboration et al.(2013)]{Robitaille:2013} Astropy Collaboration, Robitaille, T.~P., Tollerud, E.~J., et al.\ 2013, \aap, 558, A33 % 10.1051/0004-6361/201322068

\bibitem[Blanco-Cuaresma et al.(2015)]{Blanco_Cuaresma_2015} Blanco-Cuaresma, S., Soubiran, C., Heiter, U., et al.\ 2015, \aap, 577, A47 % 10.1051/0004-6361/201425232

\bibitem[Boulton \& Wallace(1969)]{Boulton_1969} Boulton, D.~M. \& Wallace, C.~S.\ 1969, Journal of Theoretical Biology, 23, 269-278 

\bibitem[Conway \& Sloane(1984)]{Conway_1984} Conway, J.~H. \& Sloane, N.~J.~A.\ 1984, Journal on Algebraic and Discrete Methods, 5, 3, 294-305 % 10.1137/0605031

\bibitem[Dalton et al.(2012)]{weave} Dalton, G., Trager, S.~C., Abrams, D.~C., et al.\ 2012, \procspie, 8446, 84460P % 10.1117/12.925950

\bibitem[de Jong et al.(2012)]{4most} de Jong, R.~S., Bellido-Tirado, O., Chiappini, C., et al.\ 2012, \procspie, 8446, 84460T % 10.1117/12.926239

\bibitem[De Silva et al.(2015)]{DeSilva_2015} De Silva, G.~M., Freeman, K.~C., Bland-Hawthorn, J., et al.\ 2015, \mnras, 449, 2604 % 10.1093/mnras/stv327

%Some applications of matrix derivatives in multivariate analysis.

\bibitem[Dwyer (1967)]{Dwyer_1967} Dwyer, P.~S.\ 1967, Journal of the American Statistical Association, 62, 318, 607-625

\bibitem[Foreman-Mackey(2014)]{Foreman_Mackey_2014} Foreman-Mackey, D.\ 2014, Blog Post: Mixture Models., Zenodo, https://doi.org/10.5281/zenodo.15856 % 10.5281/zenodo.15856

\bibitem[Freeman \& Bland-Hawthorn(2002)]{Freeman_2002} Freeman, K., \& Bland-Hawthorn, J.\ 2002, \araa, 40, 487 % 10.1146/annurev.astro.40.060401.093840

\bibitem[Gilmore et al.(2012)]{Gilmore_2012} Gilmore, G., Randich, S., Asplund, M., et al.\ 2012, The Messenger, 147, 25 % 2012Msngr.147...25G

\bibitem[Hogg et al.(2016)]{Hogg_2016} Hogg, D.~W., Casey, A.~R., Ness, M., et al.\ 2016, \apj, 833, 262 % 10.3847/1538-4357/833/2/262

\bibitem[Lada(2010)]{Lada_2010} Lada, C.~J.\ 2010, Philosophical Transactions of the Royal Society of London Series A, 368, 713 % 10.1098/rsta.2009.0264

\bibitem[Majewski et al.(2015)]{apogee} Majewski, S.~R., Schiavon, R.~P., Frinchaboy, P.~M., et al.\ 2015, arXiv:1509.05420 % 2015arXiv150905420M

\bibitem[Magnus \& Neudecker(1988)]{Magnus_1988} Magnus, J.~R. \& Neudecker, H.\ 1998, Matrix differential calculus with applications in statistics and econometrics. Wiley, New York.

\bibitem[Mitschang et al.(2013)]{Mitschang_2013} Mitschang, A.~W., De Silva, G., Sharma, S., \& Zucker, D.~B.\ 2013, \mnras, 428, 2321 % 10.1093/mnras/sts194

\bibitem[Mitschang et al.(2014)]{Mitschang_2014} Mitschang, A.~W., De Silva, G., Zucker, D.~B., et al.\ 2014, \mnras, 438, 2753  % 10.1093/mnras/stt2320

\bibitem[Oliver et al.(1996)]{Oliver_1996} Oliver, J.~J., Baxter, R.~A., Wallace, C.~S.\ 1996, Unsupervised Learning using MML, Proceedings of the 13th International Conference on Machine Learning, Morgan Kaufmann, 364-372 

\bibitem[Randich et al.(2013)]{Randich_2013} Randich, S., Gilmore, G., \& Gaia-ESO Consortium 2013, The Messenger, 154, 47 % 2013Msngr.154...47R

\bibitem[Roberts et al.(1998)]{Roberts_1998} Roberts, S., Husmeier, D., Rezek, I., Penny, W.\ 1998, Bayesian approaches to Gaussian mixture modeling. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20, 11, 1133-1142

\bibitem[Shannon(1948)]{Shannon_1948} Shannon, C.~E.\ 1948, Bell System Technical Journal, 27, 3 % 10.1002/j.1538-7305.1948.tb01338.x

\bibitem[Schafer(1997)]{Schafer_1997} Schafer, J.~L.\ 1997, Analysis of Incomplete Multivariate Data (CRC Press) % 10.1002/(SICI)1097-0258(20000415)19:7<1006::AID-SIM384>3.0.CO;2-T

\bibitem[Wallace \& Boulton(1968)]{Wallace_1968} Wallace, C.~S. \& Boulton, D.~M.\ 1968, The Computer Journal, 11, 2 %10.1093/comjnl/11.2.185

\bibitem[Wallace \& Freeman(1987)]{Wallace_1987} Wallace, C.~S. \& Freeman, P.~R.\ 1987, Journal of the Royal Statistical Society. Series B (Methodological), 49, 3, 240-265

\bibitem[Wallace(2005)]{Wallace_2005} Wallace, C.~S.\ 2005, Statistical and inductive inference using minimum message length. Springer-Verlag, NJ, USA

\end{thebibliography}

\end{document}






\begin{comment}
\subsection{Problems using real world data}

There are a number of problems to consider when using real world data.

\begin{enumerate}
  \item There are missing data.
        Given high-resolution spectra with signal-to-noise (S/N) ratios of
        $\infinity$, there will be chemical abundances where only an upper
        limit can be reported.
        For example, if there are no transitions in the spectra, or the
        chemical abundance is so low that there is no net absorption in the 
        photosphere, then there will be no observable effect in the data.
        Chemical tagging approaches must be able to accept `upper limits'\footnote{
          The use of the term `\emph{upper limit}' amongst spectroscopists is
          muddled. 
          Frequently an `upper limit' is reported that represents (near-)zero
          likelihood that the abundance could be above the quoted value.
          Alternatively, a reported `upper limit' might describe a 
          $\approx1\sigma$ (non-symmetric) limit estimated from the local
          noise in the data.
          Ideally a posterior distribution should to be provided.
        }
        reported by spectroscopists.
  \item The abundance measurements may be biased.
  \item If there are uncertainties provided for abundances, then those
        uncertainties are likely to be underestimated.
  \item The measurements will have heteroscedastic noise properties.
        In other words, the abundance uncertainties will differ between
        elements and stars.
  \item The measurements of individual chemical abundances may be correlated
        due to the way the abundance was measured.
        For example, if there is a single absorption line present in the
        spectrum for an element (e.g., \ion{O}{1}), and that absorption line
        is blended with a comparably strong transition from another element
        (e.g., \ion{Ni}{1}), then the measured abundances of \ion{O}{1} and
        \ion{Ni}{2} will be (positively) correlated.
        This correlation may be difficult to discern from an \emph{intrinsic}
        correlation between the two elements.
\end{enumerate}
\end{comment}
