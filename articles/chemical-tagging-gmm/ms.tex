%% This document is part of the snob project
%% Copyright 2017 the authors. All rights reserved.

\documentclass{aastex61}

\usepackage{bm}
%\usepackage{boldsymbol}
\usepackage{verbatim} % for \begin{comment}'ing out sections.

% For revision history
\IfFileExists{vc.tex}{\input{vc.tex}}{
    \newcommand{\githash}{UNKNOWN}
    \newcommand{\giturl}{UNKNOWN}}

% Define commands
\newcommand{\article}{\emph{Article}}
\newcommand{\acronym}[1]{{\small{#1}}}
\newcommand{\project}[1]{\textsl{#1}}

% Surveys
\newcommand{\apogee}{\acronym{APOGEE}}
\newcommand{\ges}{\acronym{GES}}
\newcommand{\hermes}{\acronym{HERMES}}
\newcommand{\galah}{\acronym{GALAH}}
\newcommand{\fourmost}{\acronym{4MOST}}
\newcommand{\weave}{\acronym{WEAVE}}
\newcommand{\gaia}{\project{Gaia}}

% Common terms
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}

\def\teff{T_{\rm eff}}
\def\logg{\log{g}}

% Have some maaaath.
\def\infinity{\rotatebox{90}{8}}
\def\veccov{\vect{C}}
\def\vecmean{\vect{\mu}}
\def\vectheta{\vect{\theta}}
\def\weight{w}
\def\weights{\vect{\weight}}
\def\datum{y}
\def\data{\vect{\datum}}
\def\likelihood{\mathcal{L}}

% Affiliation(s)
\newcommand{\moca}{
	\affil{School of Physics and Astronomy, Monash University, 
		   Melbourne, Clayton VIC 3800, Australia}}
\newcommand{\claytonfit}{
	\affil{Faculty of Information Technology, Monash University,
	       Melbourne, Clayton VIC 3800, Australia}}
\newcommand{\caulfieldfit}{
	\affil{Faculty of Information Technology, Monash University,
		   Melbourne, Caulfield East VIC 3145, Australia}}



\received{}
\revised{}
\accepted{}

\submitjournal{TBD} % ApJ, MNRAS, PASA?

\shorttitle{Chemical Tagging Using Minimum Message Length. I.}
\shortauthors{Casey et al.}

\begin{document}

\title{Chemical Tagging Using Minimum Message Length. I.\\
       Gaussian Mixture Modelling}

\correspondingauthor{Andrew R. Casey}
\email{andrew.casey@monash.edu}

\author[0000-0003-0174-0564]{Andrew R. Casey}
\moca
\claytonfit

\author{(some order of:}

\author[0000-0003-2952-859X]{John Lattanzio}
\moca

\author[0000-0002-0583-5918]{David Dowe} 
\claytonfit

\author[0000-0002-1716-690X]{Aldeida Aleti}
\caulfieldfit

\author{)}

\author{others?}
 
\begin{abstract}
Chemical tagging seeks to identify co-natal stars by their present-day 
photospheric abundances, long after their phase space similarity is lost.
In principle the detailed chemical abundances of a transformative number of
stars could be used to reconstruct the evolution of the Milky Way, providing
insight on supernovae yields, galactic dynamics (e.g., constraints on radial 
mixing), the initial mass function, as well as the formation and destruction
of star clusters. Some progress has been made towards chemical tagging, but 
here we argue that even the \emph{extent} of the problem has not been realised.
We introduce the Real World problems associated with chemical tagging at scale,
and describe how currently considered methods will inevitably fail even under
the most unrealistically optimistic conditions. We introduce the minimum message
length to the astronomical community as a promising alternative to chemical
tagging, one which has the capability to address all ensivaged problems with
chemical tagging simultaneously. We perform chemical tagging experiments with
infinte Gaussian mixture models, and demonstrate using real and generated data 
that MML outperforms all other considered penalty techniques. 
\end{abstract}

\keywords{}

\section{Introduction} 
\label{sec:introduction}

\citet{Freeman_2002} introduced the idea of chemical tagging to identify 
groups of stars that formed together in the same gas cloud, which are no 
longer identifiable from their physical proximity or orbital properties.
This idea is attractive because the observable chemical abundances of 
stars remains largely unchanged throughout a star's lifetime, whereas phase 
space similarity can be quickly washed out through dynamical interactions. 
Indeed, although most stars are thought to form in clusters \citep{someone},
only the most massive or isolated star clusters are able to survive the star
formation process \citep{someone2}, leaving most clusters to become 
dynamically unbound after about 1~Gyr. 

%Their signature of formation is locked up in their detailed chemical fingerprint



%The Milky Way is our best laboratory for understanding galaxy formation and
%evolution. 
Making strong inferences about the Milky Way's formation and evolution from
chemical tagging will require a transformative number of stars with precise
abundances.
Ground- and space-based surveys are poised to deliver those data this decade. 
The Apache Point Observatory Galactic Evolution Experiment 
\citep[\apogee, e.g.,][]{apogee} and the Gaia-ESO survey 
\citep[\ges;][]{Gilmore_2012,Randich_2013} are already delivering $10^5$ stars 
with 20--40 precise abundances, and the Galactic Archaeology with \hermes\ 
\citep[\galah;][]{DeSilva_2015} survey seeks to derive $\approx$30 precise 
abundances for $\sim10^6$ stars. 
Future ground-based surveys will derive abundances from $\approx$20 million 
spectra in each hemisphere \citep[e.g., \weave, \fourmost;][]{weave,4most},
and the radial velocity spectrometer on \gaia\ can deliver a few abundances 
for $\approx$150 million stars.


Chemical tagging is a simple idea, but with this volume of data it represents
a formidable parameter estimation and model selection problem.
Before reviewing the literature on chemical tagging, here we explicitly
formulate the scope of the analysis problem.


\subsection{What do we want to infer from chemical abundances?}
\label{sec:the-problem}


Let us assume that a set of stars have been observed spectroscopically, and
chemical abundances have been derived from those data.
We will unrealistically assume that the data are noiseless, and the derived
abundances are unbiased, known with infinite precision, and complete (e.g., no 
missing abundances).
Regardless of the number of stars observed or abundances derived, there are a
number of inferences we seek to make:


\begin{enumerate}
  \item The number of star clusters (or star-forming events) where at least 
        one star was observed and chemical abundances were derived.
  \item The conditional probabilities (or memberships) of each star belonging
        to every star cluster or star-forming event.
        Alternatively, the relative weighting of individual mixtures.
  \item The mean abundances of each star cluster. 
  \item The covariance matrix of abundances of each star cluster.
        This covariance matrix includes the intrinsic (cluster) variance
        of individual abundances (e.g., the homogeniety), and allows for
        correlations between abundances.\footnote{
          It is likely -- or hoped -- that \emph{true} correlations between 
          abundances will be accounted for through the latent factors, and any 
          remaining correlations are the result of the measurements.}
  \item The \emph{number} of common multivariate latent factors, variables 
        that are not observable but affect all or many of the observations.
        In the context of chemical tagging, the yields of core-collapse
        supernovae can be considered a common multivariate latent factor: 
        those yields are not directly observable, but they contribute to much 
        of the data.
        Here we are interested in the \emph{number} of common multivariate
        latent factors. For example, the number of sources of enrichment:
        different classes of supernovae, asymptotic giant branch (AGB) stars, 
        etc.
  \item The multivariate factor loads of individual latent factors. 
        For example, if core-collapse supernovae and AGB stars had both 
        contributed to the chemical abundances of all stars in the sample, 
        then the \emph{factor loads} would be the \emph{yield} of the 
        supernovae, or the yield of relative abundances that were produced by 
        the AGB star. 
        Each star will be affected differently by those yields (see item 7).
  \item The relative \emph{scoring} of different latent factors on each star.
        This is a scaling that is applied to the latent yields. Continuing our
        analogy on yields from core-collapse supernovae and AGB stars, the
        \emph{factor scores} can be thought of as the relative contributions 
        of the different factor loads, for a single star.
  \item The intrinsic \emph{specific variances} of the individual abundances.
        These variances can be thought of as the intrinsic variability from 
        the different factor loads.
        The intrisic variances affect all observations, before accounting for
        any homogeneity in individual clusters.
  \item The \emph{number} of stellar age- or parameter-dependent latent 
        factors that affect the data. 
        The surface chemical abundances of stars \emph{do change} over its
        lifetime, at least in part due to atomic diffusion, veiling, and 
        thermohaling mixing.
        Understanding and modelling these effects will be critical for 
        chemical tagging stars across all stellar ages and evolutionary states.
  \item The factor loads of stellar age- or parameter-dependent latent factors.
        In this example, a single multivariate factor load may represent the
        age-dependent effects of atomic diffusion.
  \item The factor scores of stellar age- or parameter-dependent latent 
        factors.
\end{enumerate}


The number of things we seek to infer from the available chemical abundances
is vast.
Indeed, even if we ignore the latent factors then a non-trivial model
selection problem remains: What is the number of star clusters or star-forming 
events?
In the full description of chemical tagging we seek to know (1) the number of
star clusters, (2) the number of latent factors, and (3) the number of age-
or stellar parameter-dependent factors.
Going further, if we wanted to improve the accuracy and precision of our
solution then we may seek to include joint priors on the memberships of stars
being associated with specific clusters, either based on their ages or
astrometry.
In summary, simplified versions of chemical tagging represent formidable model
selection problems with non-convex\footnote{
  If a problem is convex then any local solution is mathematically guaranteed
  to be the global solution.
  If the problem is non-convex, then a local solution may not be the global
  solution.
} objective functions.


\subsection{Chemical tagging progress to date}
\label{sec:literature-review}


There has been substantial progress on chemical tagging in the last decade.
Given the difficulty of the problem, most studies have opted to address one
(or a few) of the challenges listed in Section~\ref{sec:the-problem}. 
The most common issue addressed is group-finding (items 1-4). 
Some works have sought to construct a similarity metric \citep{Mitschang}
based on chemical abundances of undisrupted clusters. 
Others have applied group-finding algorithms \citep{Hogg,Gregor}.

% TODO
- Dimensionality of chemical abundance space.

- Chemical homogeneity.

- Dopplegangers.


The aforementioned chemical tagging experiments have sought to address only 
individual challenges associated with chemical tagging.
These challenges are all related, and therefore addressing them separately
will yield results that are not self-consistent, and do not make use of all
the information available.


In this \article\ we will introduce a simple probabilistic approach to
chemical tagging, where the data are modelled as a mixture of multivariate
Gaussian distributions and the number of mixtures is unknown.
In Section \ref{sec:the-model} we describe our model and outline our objective
function.
Although our model is simplistic in the sense that we will only address some
aspects of chemical tagging, in Section \ref{sec:experiments} our experiments
show that our approach is superior to all other methods considered 
for chemical tagging.
In Section \ref{sec:discussion} we describe how the principles introduced here
can be used to construct a fully consistent probabilistic approach to chemical
tagging, and simultaneously address the challenges outlined in Section 
\ref{sec:the-problem}.


\section{The Model}
\label{sec:the-model}


We make the following explicit assumptions:

\begin{itemize}
  \item We assume that the data (e.g., $D$ detailed chemical abundances for
        $N$ stars) can be represented by a mixture of $K$ multivariate 
        Gaussian distributions.
  \item We assume that there are no missing data values. 
        For example, if a star has been observed, then there is a complete set
        of $D$ chemical abundances available.
  \item We will assume that the number of \emph{true} multivariate Gaussian
        distributions $K_{true}$ is not known. 
        This assumption applies to our experiements involving generated data, 
        and those using real data.
  \item In addition to not knowing the \emph{number} of multivariate normal
        distributions, we further assume that the means $\vecmean$ and 
        covariance matrices $\veccov$ of each $K$-th mixture is unknown.
  \item We assume that the covariance matrix of individual components can be
        described as `free' or `full', insomuch that the data within a cluster
        can have off-diagonal correlation terms, and those terms are unknown.
  \item In all experiments we will assume that the data have homoskedastic 
        noise properties.
  \item The relative weights $\weights$ (or mixing proportions) of the $K$ 
        distributions is unknown. 
  \item We assume a relative (or conditional) probability distribution for 
        each star belonging to a given cluster. That is to say that we do not 
        adopt a `\emph{hard selection}' approach to cluster modelling, where 
        stars would be assigned as definitively belonging to one cluster or 
        another. 
        Our approach could be described as `\emph{soft} clustering'.
        This assumption has practical implications for optimization, and for
        the total message length.
\end{itemize}


\noindent{}The probability density function $p$ for a multivariate normal 
distribution with $D$ dimensions is given by

\begin{equation}
  p(\data|\vecmean,\veccov) 
      = \frac{1}{\sqrt{(2\pi)^d|\veccov|}}
        \exp{\left[-\frac{1}{2}(\data - \vecmean)^\intercal\veccov^{-1}(\data - \vecmean)\right]}
\end{equation}

\noindent{}where $\vecmean$ and $\veccov$ is the multivariate mean and 
covariance matrix, and $\data$ are the data.
For a fixed number of $K$ multivariate Gaussian mixtures, the probability
density function can be written as

\begin{equation}
  p(\data|\vectheta) = \sum_{k=1}^{K} \weight_k \, p(\data|\vectheta_k)
\end{equation}

\noindent{}where $\vectheta_k \equiv \{\vecmean_k, \veccov_k\}$,
$\vectheta \equiv \{\vectheta_1,\dots,\vectheta_K,\weight_1,\dots,\weight_K\}$, and
$\weight_k$ is the relative weight (or the mixing probability) of the $k$-th 
mixture,\footnote{
  An alternative representation would be to include integer variables for each
  of the $N$ stars, where each variable represents the cluster which that star
  belongs.
  These two models are mathematically identical \citep{Foreman_Mackey_2014},
  but the integer representation is highly dimensional and difficult to
  optimize.}
and we require that $\sum_{k=1}^{K} \weight_k = 1$.
Although the true number of $K$ mixtures is unknown (as are the means $\vecmean$
and covariance matrices $\veccov$), we will address the selection of $K$
and parameter estimates of $\vectheta$ in Section \ref{sec:fitting-the-data}.


\subsection{Minimum Message Length}
\label{sec:mml}

The Minimum Message Length \citep[MML;][]{Wallace_1968} principle is a formal
description of Occam's razor that is based on information theory.
The MML principle states that the best explanation of the data is the model
that describes the data using the smallest amount of information (shortest
message).
Information is described with the same description as \citet{Shannon_1948},
who showed that that given an event $E$ with probability $P(E)$, the length of 
the shortest lossless code $I(E)$ to represent that event requires 
$I(E) = -\log_{2}P(E)$ bits of information\footnote{
  Unless specified -- as it is in this instance -- throughout this article we 
  will refer to the information $I$ or entropy in natural units (nats). 
  Recall that 1 bit $= \log{2}$ nats.
  }.
\citet{Wallace_1968} linked this result with Bayes' theorem, 

\begin{equation}
  P(H|D) = \frac{P(H)\cdot{}P(D|H)}{P(D)}
\end{equation}

\noindent{}which states that the joint probability of the data $D$ and 
hypothesis $H$, $P(H, D)$, is given by

\begin{equation}
  P(H, D) = P(H) \cdot P(D|H) = P(D) \cdot P(H|D)
\end{equation}

\noindent{}where $P(H)$ is the prior probability of the hypothesis $H$,
$P(D|H)$ is the likelihood, $P(D)$ is the prior probability of the data $D$,
and $P(H|D)$ is the posterior probability of the hypothesis given the data.
Specifically, \citet{Wallace_1968} derived the relationship between
conditional probabilities in terms of optimal message lengths,

\begin{equation}
  I(H, D) = I(H) + I(D|H) = I(D) + I(H|D) \quad .
  \label{eq:mml-wallace}
\end{equation}

Therefore the total cost of a message is the sum of the message length of the
hypothesis $H$, which takes $I(H)$ bits, and the data $D$ given the hypothesis
$H$, $I(D|H)$. 
Considering a simple problem of model selection provides good intuition for
this two-part message approach. 
Consider a simple model $H_0$ where the first part of the message $I(H_0)$ is 
short, and which provides a poor fit to the data such that $I(D|H_0)$ is long.
An alternative model $H_1$ is more complex, requiring a longer $I(H_1)$, but
the increased model flexibility provides a better fit to the data, making
$I(D|H_1)$ short.
For both parameter estimation \emph{and} model selection, we want to chose
$H$ (of $H_0$, $H_1$, etc) and the model parameters that minimizes the
\emph{total} message length.



Calculating the length of the message is a non-trivial task, especially if the
model is even reasonably complex.
This makes the MML principle intractable (or uncomputable) in most cases, and
forces us to make approximations when calculating the message length.
\citet{Wallace_Freeman_1987} introduce a generalised scheme to estimate a
vector of parameters $\vectheta$ of any distribution given some data, where
the estimates $\vectheta$ that minimise the message length expression,

\begin{equation}
  I(\vectheta,\data) 
    = \frac{p}{2}\log{q_p} - \log{\left(\frac{h(\vectheta)}{\sqrt{|\mathcal{F}(\vectheta)|}}\right)}
      - \likelihood\left(\data|\vectheta\right) + \frac{p}{2}
      \label{eq:message-length}
\end{equation}


\noindent{}is the MML estimate.
Here $h(\vectheta)$ is the prior on the hypothesis, $p$ is the number of free
parameters in the model, $q_p$ is the lattice quantization constant
\citep[e.g.,][]{Conway_1984}, and $|\mathcal{F}\left(\vectheta\right)|$ is the
determinant of the \emph{expected} Fisher information matrix (the second-order
partial derivatives of the negative log-likelihood function 
$-\likelihood(\data|\vectheta)$).




%The message length for a model includes the length (in bits of entropy)
%required to transmit the model parameters, and the length required to transmit
%the residuals of the data given the model.
%There is a wealth of literature on the successes (and derivatives) of MML
%in the fields of medicine, statistics, etc.

%- Deep grounds in statistics, etc.


The MML principle requires that the prior beliefs of the model to be fully 
specified as part of the message. 
We must explicitly specify our prior beliefs on the model parameters described
in Section \ref{sec:model} in order to minimize the full message length.
Indeed, the message must fully specify all components necessary for the
receiver to be able to reconstruct the message.
For the model specified in Section \ref{sec:model}, this requirement forces us
to specify a number of components:

\begin{enumerate}
  \item The number of Gaussian mixtures, $K$.
  \item Encoding the relative weights $\weights$ of the Gaussian mixtures.
  \item Encoding the component parameters $\vecmean$, $\veccov$ for all of
        the Gaussian mixtures.
  \item Encoding the data, given the model parameters.
  \item The lattice quantization constant $q_p$ for the number of model
        parameters $p$.
\end{enumerate}

Some of these components require non-trivial derivations or approximations in
order to calculate the length of the message required to encode the component.
For this reason, we will consider the message length of each component in turn
before returning to the full message length specified in Equation \ref{eq:message-length}.


\subsubsection{Encoding the number of Gaussian mixtures, $K$}
\label{sec:encoding-k}


We assume that fewer mixtures are more likely than a larger number of mixtures.
Specifically, we assume that $p(K) \propto 2^{-K}$, which implies that the
message length required to encode $K$ is given by:

\begin{equation}
  I(K) = -\log{p(K)} = K\log{2} + \textrm{constant} \quad .
\end{equation}


\subsubsection{Encoding the relative weights, $\weights$}
\label{sec:encoding-weights}

We assume a uniform prior on the mixing weights $\bm{w}$, only requiring that
$\sum_{k=1}^{K}w_k = 1$.
% This implies the weights are parameters of a multinomial distribution.
\citet{Boulton_1969} give the following expression for the message length 
required to encode the weights,

\begin{equation}
I(\weights) = \frac{K - 1}{2}\log{N} - \frac{1}{2}\sum_{k=1}^{K}\log\weight_k - \log{\Gamma{\left(K\right)}} \quad ,
\end{equation}

\noindent{}where $\Gamma(K)$ is the usual gamma function for positive integers
$\Gamma(K) = (K - 1)!$.
A useful extension of this work might be to impose a Dirchlet prior on the 
mixing weights $\weights$, or a distribution inspired by the stellar mass 
function and/or an observed selection function.

%This requirement provides a familiar link to Bayesian statistics, and allows
%one to think of MML as a form of a Bayesian optimization problem that is 
%deeply grounded in information theory, with successful contributions in
%economics, medicine, statistics, etc \citep{citations_needed}.


\subsubsection{Encoding the mixture parameters $\vecmean$ and $\veccov$}
\label{sec:encoding-mixture-parameters}

Recall that what we need to encode is both our priors on the model parameters
and the determinant of the expected fisher information matrix.
For the $k$-th mixture this becomes,

\begin{equation}
  I(\vecmean_k,\veccov_k) = -\log{\left(\frac{p(\vecmean_k,\veccov_k)}{\sqrt{|\mathcal{F}\left(\vecmean_k,\veccov_k\right)|}}\right)}
                          = -\log{p(\vecmean_k,\veccov_k)} + \frac{1}{2}\log{|\mathcal{F}\left(\vecmean_k,\veccov_k\right)|}
\end{equation}

\noindent{}and for all mixtures:

\begin{equation}
I(\vecmean,\veccov) = -\sum_{k=1}^{K}\log{p(\vecmean_k,\veccov_k)} + \frac{1}{2}\sum_{k=1}^{K}\log{|\mathcal{F}\left(\vecmean_k,\veccov_k\right)|}
\end{equation}

This requires us to explicitly specify our priors on the parameters $\vecmean$
and $\veccov$.
We adopt an improper uniform prior of 
$\mathcal{U}(\vecmean) = [-\infinity, \infinity]$ on all multivariate mean 
abundances $\vecmean$ (in all $D$ dimensions).
This prior has no impact on our inferences and becomes proper when it enters 
the (accepted) posterior distribution. 
We similarily could adopt a proper and large uniform prior on all mean 
abundances of $\mathcal{U}(\vecmean) = [-12, 12]$ (24 orders of magnitude!) 
and the result would be the same.


We adopt a congujate inverted Wishart prior for the covariance matrices in the
individual mixtures. 
The joint prior density for the parameters of a single
mixture is given by the limiting form of the normal inverted-Wishart
density \citep[e.g., Section 5.2.3 of ][]{Schafer_1990}:

\begin{equation}
    p(\vecmean_k,\veccov_k) \propto |\veccov_k|^{\frac{D + 1}{2}} \quad .
\end{equation}


Computing the expected Fisher information $\mathcal{F}$ requires the second 
order partial derivatives of the negative log-likelihood 
$-\likelihood(\data|\vecmean,\veccov)$.
We approximate the determinant of the Fisher information 
$|\mathcal{F}(\vecmean_k, \veccov_k)|$ as the product of 
$|\mathcal{F}\left(\vecmean_k\right)|$ and $|\mathcal{F}\left(\veccov_k\right)|$ 
\citep{Oliver_1996,Roberts_1998}. 
We begin by taking second derivative of the log-likelihood function
$\likelihood\left(\data|\vecmean,\veccov\right)$ for a multivariate Gaussian
distribution,

\begin{equation}
  \likelihood\left(\data|\vecmean, \veccov\right) = 
  -\frac{Nd}{2}\log{\left(2\pi\right)} - \frac{N}{2}\log{|\veccov|} - \frac{1}{2}\Sigma_{i=1}^{N}\left(\data_i - \vecmean\right)^\intercal{}\veccov^{-1}\left(\bm{y}_i - \vecmean\right)
\end{equation}

\noindent{}with respect to $\vecmean$,

\begin{eqnarray}
  \frac{\delta\likelihood}{\delta\vecmean} = \Sigma_{i=1}^{N}\veccov^{-1}(\data_i - \vecmean)
  & \hspace{1em}\textrm{and}\hspace{1em}
  & \frac{\delta^2\likelihood}{\delta\vecmean^2} = -N\veccov^{-1} \quad ,
\end{eqnarray}

\noindent{}and for a $d$-dimensional vector $\vecmean$ the Fisher information
matrix is:

\begin{eqnarray}
  |\mathcal{F}(\vecmean_k)| = \left|-\frac{\delta^2\likelihood}{\delta\vecmean^2}\right| = N^d|\veccov|^{-1} \quad .
\end{eqnarray}

Computing the Fisher information of the covariance matrix 
$|\mathcal{F}\left(\veccov\right)|$ is harder. 
For a full covariance matrix with off-diagonal terms, the total number of free 
parameters required is $d(d+1)/2$.
\citep{Magnus_1988} derived an analytical expression:

% Dwyer_1967, Bozdogan_1990? Drton_2009?
% TODO: This derivation is a nightmare to find

\begin{equation}
  |\mathcal{F}\left(\veccov\right)| = N^\frac{d(d+1)}{2}2^{-d}|\veccov|^{-(d+1)}
\end{equation}

\noindent{}Giving the approximate determinant of the expected Fisher information 
$|\mathcal{F}\left(\vecmean,\veccov\right)|$ as:

\begin{eqnarray}
  |\mathcal{F}\left(\vecmean,\veccov\right)| & \approx & |\mathcal{F}\left(\vecmean\right)|\cdot|\mathcal{F}\left(\veccov\right)| \nonumber \\
  |\mathcal{F}\left(\vecmean,\veccov\right)| & \approx & N^{d}|\veccov|^{-1}N^\frac{d(d+1)}{2}2^{-d}|\veccov|^{-(d+1)} \nonumber \\
  |\mathcal{F}\left(\vecmean,\veccov\right)| & \approx & N^\frac{d(d+3)}{2}2^{-d}|\veccov|^{-(d+2)} \quad .
\end{eqnarray}

Thus for all $K$ mixtures, Equation \ref{eq:10} expands to:

\begin{eqnarray}
I(\vecmean, \veccov) &=& -\sum_{k=1}^K\frac{D + 1}{2}\log{|\veccov_k|} 
                         + \frac{1}{2}\sum_{k=1}^K\left[\frac{D(D+3)}{2}\log{N} - D\log{2} - (D + 2)\log{|\veccov_k|}\right] \nonumber \\
I(\vecmean, \veccov) &=& -\sum_{k=1}^K\frac{D + 1}{2}\log{|\veccov_k|}
                         + \frac{KD(D + 3)}{4}
                         + \frac{D\log{2}}{2}
                         - \frac{1}{2}\sum_{k=1}^K (D+2)\log{|\veccov_k|} \nonumber\\
I(\vecmean, \veccov) &=& -\sum_{k=1}^K(D + 1)\log{|\veccov_k|} + \frac{1}{2}\left(\frac{KD(D + 3)}{2} + D\log{2}\right)
\end{eqnarray}



\subsubsection{Encoding the data, given the model parameters}
\label{sec:encoding-data}


Each datum can only be stated with a precision which is given by the accuracy
of the measurement.
If we assume homoskedastic noise properties for the observed data, and the
precision that each elemental abundance can be measured is $\epsilon$, then
the \emph{probability} of a datum is given by 
$\textrm{Pr}(\datum_n) = \epsilon^D\textrm{Pr}(\datum_i|\mathcal{M})$
where $\mathcal{M}$ represents the model and $\textrm{Pr}(\datum_i|\mathcal{M})$
is the \emph{probability density}.
Note that $\epsilon$ is a constant, and is only used to maintain validity 
between \emph{probability} and \emph{probability density}.
For these experiments we adopt $\epsilon = 0.01$, but note that the value of
$\epsilon$ has no effect on our inferences.

The length of the total encoding of a datum given the model is,
\begin{equation}
  I(\data_n) = -\log{\textrm{Pr}(\data_n)} = -D\log\epsilon - \log\sum_{k=1}^{K}w_{k}f_{k}(\data_n|\vecmean_k,\veccov_k)
\end{equation}

\noindent{}and for the entire data:

\begin{equation}
  I(\data|\vectheta) = -ND\log\epsilon - \sum_{n=1}^{N}\log\sum_{k=1}^{K}w_{k}f_k(\data_n|\vecmean_k,\veccov_k)
\end{equation}


\subsubsection{Encoding the lattice constant}



\subsection{The Objective Function}
\label{sec:objective-function}


The message lengths of individual components allows us to fully specify an
objective function to minimise the total message length, given any number of
Gaussian mixtures $K$ and parameter estimates $\vectheta$:

\begin{equation}
% TODO:
  I(\vectheta,\data) = I(K) + I(\weights) + \sum_{k=1}^{K}I(\vectheta_k) + I(\data|\vectheta) + \textrm{lattice} \quad .
\end{equation}

Substituting the expressions outlined in previous subsections, we arrive at:

\begin{equation}
  % FULL EQUATION.
  I(\vectheta,\data) = long
\end{equation}

Note that our objective function includes the number of mixtures $K$, a
discrete valued parameter. 
Because $K$ is not continuous, this complicates our optimization procedure.


Here we will outline our optimization procedure to minimise the message length
when $K$ is known and fixed, before briefly describing our search strategy to
move between trial values of $K$.



\section{Experiments}
\label{sec:experiments}

Here we describe simple chemical tagging experiments that are so optimistic
that they could be considered laughable, but ones which will still fail for
even the most simplistic chemical tagging approaches.

Each experiment will edge closer to a representative example of chemical
tagging.

\subsection{Experiment 0: Position and Velocity information}

\subsection{Experiment 1: Using clusters, full complement of chemical abundances}

\subsection{Experiment 2: Using all clusters, full complement of chemical abundances, random fractions of field stars}

\subsection{Experiment 3: Using clusters, limited set of chemical abundances}

\subsection{Experiment 4: Fake data to replicate clusters, but use latent factor}

\subsection{Experiment 5: Fake data to replicate clusters, but use multiple latent factors}


\section{Discussion}
\label{sec:discussion}


\acknowledgments

This research has made use of NASA's Astrophysics Data System.
This research is supported by an Australian Research Council Discovery Project
grant (DP160100637). 
Source code for this project is available at \texttt{\giturl}. This document
was compiled on \gitdate\ from revision hash \texttt{\githash} in that
repository. 


\software{astropy \citep{Robitaille:2013}, numpy, scipy, scikit-learn} 

\begin{thebibliography}{}

% Note: I include the DOI as a comment on the same line after every \bibitem
%       entry. I do this in case I ever need to extract full bibstems for the
%       bibliography, which can be done using this curl command and the DOI:

%       curl -LH "Accept: application/x-bibtex" http://dx.doi.org/10.5555/12345678 

%       If no DOI is available then I list the ADS identifier.

\bibitem[Astropy Collaboration et al.(2013)]{Robitaille:2013} Astropy Collaboration, Robitaille, T.~P., Tollerud, E.~J., et al.\ 2013, \aap, 558, A33 % 10.1051/0004-6361/201322068

\bibitem[Dalton et al.(2012)]{weave} Dalton, G., Trager, S.~C., Abrams, D.~C., et al.\ 2012, \procspie, 8446, 84460P % 10.1117/12.925950

\bibitem[de Jong et al.(2012)]{4most} de Jong, R.~S., Bellido-Tirado, O., Chiappini, C., et al.\ 2012, \procspie, 8446, 84460T % 10.1117/12.926239

\bibitem[De Silva et al.(2015)]{DeSilva_2015} De Silva, G.~M., Freeman, K.~C., Bland-Hawthorn, J., et al.\ 2015, \mnras, 449, 2604 % 10.1093/mnras/stv327

\bibitem[Freeman \& Bland-Hawthorn(2002)]{Freeman_2002} Freeman, K., \& Bland-Hawthorn, J.\ 2002, \araa, 40, 487 % 10.1146/annurev.astro.40.060401.093840

\bibitem[Gilmore et al.(2012)]{Gilmore_2012} Gilmore, G., Randich, S., Asplund, M., et al.\ 2012, The Messenger, 147, 25 % 2012Msngr.147...25G

\bibitem[Majewski et al.(2015)]{apogee} Majewski, S.~R., Schiavon, R.~P., Frinchaboy, P.~M., et al.\ 2015, arXiv:1509.05420 % 2015arXiv150905420M

\bibitem[Randich et al.(2013)]{Randich_2013} Randich, S., Gilmore, G., \& Gaia-ESO Consortium 2013, The Messenger, 154, 47 % 2013Msngr.154...47R

\bibitem[Shannon (1948)]{Shannon_1948} Shannon, C.~E.\ 1948, Bell System Technical Journal, 27, 3 % 10.1002/j.1538-7305.1948.tb01338.x

\bibitem[Schafer (1997)]{Schafer_1997} Schafer, J.~L.\ 1997, Analysis of Incomplete Multivariate Data (CRC Press) % 10.1002/(SICI)1097-0258(20000415)19:7<1006::AID-SIM384>3.0.CO;2-T

\bibitem[Wallace \& Boulton (1968)]{Wallace_1968} Wallace, C.~S. \& Boulton, D.~M.\ 1968, The Computer Journal, 11, 2 %10.1093/comjnl/11.2.185

\end{thebibliography}

\end{document}






\begin{comment}
\subsection{Problems using real world data}

There are a number of problems to consider when using real world data.

\begin{enumerate}
  \item There are missing data.
        Given high-resolution spectra with signal-to-noise (S/N) ratios of
        $\infinity$, there will be chemical abundances where only an upper
        limit can be reported.
        For example, if there are no transitions in the spectra, or the
        chemical abundance is so low that there is no net absorption in the 
        photosphere, then there will be no observable effect in the data.
        Chemical tagging approaches must be able to accept `upper limits'\footnote{
          The use of the term `\emph{upper limit}' amongst spectroscopists is
          muddled. 
          Frequently an `upper limit' is reported that represents (near-)zero
          likelihood that the abundance could be above the quoted value.
          Alternatively, a reported `upper limit' might describe a 
          $\approx1\sigma$ (non-symmetric) limit estimated from the local
          noise in the data.
          Ideally a posterior distribution should to be provided.
        }
        reported by spectroscopists.
  \item The abundance measurements may be biased.
  \item If there are uncertainties provided for abundances, then those
        uncertainties are likely to be underestimated.
  \item The measurements will have heteroscedastic noise properties.
        In other words, the abundance uncertainties will differ between
        elements and stars.
  \item The measurements of individual chemical abundances may be correlated
        due to the way the abundance was measured.
        For example, if there is a single absorption line present in the
        spectrum for an element (e.g., \ion{O}{1}), and that absorption line
        is blended with a comparably strong transition from another element
        (e.g., \ion{Ni}{1}), then the measured abundances of \ion{O}{1} and
        \ion{Ni}{2} will be (positively) correlated.
        This correlation may be difficult to discern from an \emph{intrinsic}
        correlation between the two elements.
\end{enumerate}
\end{comment}
